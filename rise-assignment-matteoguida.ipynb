{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import all necessary libraries\n\nimport transformers\nimport torch\nimport datasets\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertConfig, BertForTokenClassification, AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification\nfrom datasets import load_dataset, load_metric, DatasetDict\nfrom transformers import TrainingArguments, Trainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T15:13:52.220246Z","iopub.execute_input":"2023-12-04T15:13:52.220704Z","iopub.status.idle":"2023-12-04T15:14:07.460453Z","shell.execute_reply.started":"2023-12-04T15:13:52.220655Z","shell.execute_reply":"2023-12-04T15:14:07.459597Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Activate GPU for training \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:14:07.461813Z","iopub.execute_input":"2023-12-04T15:14:07.463578Z","iopub.status.idle":"2023-12-04T15:14:07.491990Z","shell.execute_reply.started":"2023-12-04T15:14:07.463549Z","shell.execute_reply":"2023-12-04T15:14:07.491128Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# We needed to install the evaluation metric for this task\n\n#!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:14:10.073570Z","iopub.execute_input":"2023-12-04T15:14:10.074353Z","iopub.status.idle":"2023-12-04T15:14:25.429177Z","shell.execute_reply.started":"2023-12-04T15:14:10.074315Z","shell.execute_reply":"2023-12-04T15:14:25.427855Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.24.3)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=fd2190b04c40cb5dd98c9f3dae0cb0c5bd1a1e06c6e8df7061726745510b600b\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Import the dataset directly from HuggingFace\n\nmultinerd = load_dataset(\"Babelscape/multinerd\")","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:15:42.242893Z","iopub.execute_input":"2023-12-04T15:15:42.243282Z","iopub.status.idle":"2023-12-04T15:17:10.016153Z","shell.execute_reply.started":"2023-12-04T15:15:42.243250Z","shell.execute_reply":"2023-12-04T15:17:10.015325Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset json/Babelscape--multinerd to /root/.cache/huggingface/datasets/json/Babelscape--multinerd-d3bf0284fd817c7b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"428ebae655a143cfb2c4515520673856"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/32.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d02e8fecb2d4b7aa4146f597421b2ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/37.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae9422d4c12a4aca90fd3b5b2e0c04a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/46.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f78f6236834b168688961de088b5ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/46.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb610fe331294857803ec2076d28baab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/50.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b4689bbc91489fadf989a7a691b502"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/34.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad68c576aad4e5d86f8a93bf0d2075c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/38.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ac6fbcb26f24cfe993865a54e298feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/44.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c98171264e8f46e18bae99851748069e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/42.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c120871a742a4ef898a57608e685cb52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/60.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae7f2a9f4f8f4f5db4af6dd170cc2e7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4547e94822b470daef86ebaf5457503"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.65M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7718b9e2c00b43bca976bd3269a3af07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378cca4438564a158610afd6618deee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.28M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"469fb069476c461083b1f170ca9fee13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.51M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d35635ec7f20456c9f7a70e732bf2946"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ad2bb647f44352afb09d615227b693"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/4.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1442c092b70545869cadc89d1d89a0fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58683272054a4a5b8d6b13051159bd12"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/5.28M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b1f5bab8f44881af05065137d9cd25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/7.47M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b6af3c2e0a4b7093af4906cc74face"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69655019aa5e4973bbb0db319148ebfb"}},"metadata":{}},{"name":"stdout","text":"Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/Babelscape--multinerd-d3bf0284fd817c7b/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45faaa85b95c4f0589528053f6993325"}},"metadata":{}}]},{"cell_type":"code","source":"# Originally, two segments - train and test. Each segment contains tokens, ner_tags and information about language\nmultinerd","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:17:50.618791Z","iopub.execute_input":"2023-12-04T15:17:50.619177Z","iopub.status.idle":"2023-12-04T15:17:50.625897Z","shell.execute_reply.started":"2023-12-04T15:17:50.619134Z","shell.execute_reply":"2023-12-04T15:17:50.624948Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'lang'],\n        num_rows: 1339200\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'lang'],\n        num_rows: 167993\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Checking the dimensions of data\n\nmultinerd.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:17:52.253313Z","iopub.execute_input":"2023-12-04T15:17:52.253904Z","iopub.status.idle":"2023-12-04T15:17:52.259644Z","shell.execute_reply.started":"2023-12-04T15:17:52.253872Z","shell.execute_reply":"2023-12-04T15:17:52.258739Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'train': (1339200, 3), 'test': (167993, 3)}"},"metadata":{}}]},{"cell_type":"code","source":"# Filter out the non-English examples\n\nmultinerd = multinerd.filter(lambda sample: sample['lang'] == 'en')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:17:54.686651Z","iopub.execute_input":"2023-12-04T15:17:54.687029Z","iopub.status.idle":"2023-12-04T15:19:25.086500Z","shell.execute_reply.started":"2023-12-04T15:17:54.686995Z","shell.execute_reply":"2023-12-04T15:19:25.079785Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1340 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fcc7f71fb684a9fa16f4d40fc5c374d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/168 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9576a941b47f4fd48736bd724bccd479"}},"metadata":{}}]},{"cell_type":"code","source":"# Manually create a val set from training set, while maintaining train and test in their original form\n\ntrain_dataset = multinerd['train']\ntest_dataset = multinerd['test']\n\nsplit_ratio = 0.8\nnum_train_samples = int(len(train_dataset) * split_ratio)\n\ntrain_set = train_dataset.select(list(range(num_train_samples)))\nval_set = train_dataset.select(list(range(num_train_samples, len(train_dataset))))\n\n# Update the DatasetDict with the sets\nmultinerd = DatasetDict({\n    'train': train_set,\n    'valid': val_set,\n    'test': test_dataset\n})","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:19:37.225906Z","iopub.execute_input":"2023-12-04T15:19:37.226815Z","iopub.status.idle":"2023-12-04T15:19:37.737570Z","shell.execute_reply.started":"2023-12-04T15:19:37.226780Z","shell.execute_reply":"2023-12-04T15:19:37.736792Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"multinerd","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:19:40.096105Z","iopub.execute_input":"2023-12-04T15:19:40.096489Z","iopub.status.idle":"2023-12-04T15:19:40.102857Z","shell.execute_reply.started":"2023-12-04T15:19:40.096456Z","shell.execute_reply":"2023-12-04T15:19:40.101898Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['tokens', 'ner_tags', 'lang'],\n        num_rows: 105024\n    })\n    valid: Dataset({\n        features: ['tokens', 'ner_tags', 'lang'],\n        num_rows: 26256\n    })\n    test: Dataset({\n        features: ['tokens', 'ner_tags', 'lang'],\n        num_rows: 16454\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"# For this task, we aim for the cased base BERT tokenizer \n# Cased because capitalization most likely will play an important role in the NER task\n\n# Import the BERT tokenizer\n\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:19:44.426593Z","iopub.execute_input":"2023-12-04T15:19:44.427420Z","iopub.status.idle":"2023-12-04T15:19:48.506174Z","shell.execute_reply.started":"2023-12-04T15:19:44.427387Z","shell.execute_reply":"2023-12-04T15:19:48.505343Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dcc9802277148b28cfb4f877643739d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"244ed25badfd4b8b9b8009853b3cdc89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399699de71ff4e6b92e8e282ac5198ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb218793064f4af58b47068056aa9102"}},"metadata":{}}]},{"cell_type":"code","source":"# Word Piece tokenization of BERT causes mismatch between actual NER tags and tokens\n# As words can be split again by tokenizer (##), as exemplified in the following chunks\n\nexample = train_dataset[123]\nprint(example)\n\ntokenized_example = tokenizer(example['tokens'], is_split_into_words=True)\n\ntokens = tokenizer.convert_ids_to_tokens(tokenized_example['input_ids'])\n\nword_ids = tokenized_example.word_ids()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:15:05.754419Z","iopub.execute_input":"2023-12-04T09:15:05.754776Z","iopub.status.idle":"2023-12-04T09:15:05.764421Z","shell.execute_reply.started":"2023-12-04T09:15:05.754751Z","shell.execute_reply":"2023-12-04T09:15:05.763498Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"{'tokens': ['The', 'campaign', 'was', 'organized', ',', 'among', 'others', ',', 'by', 'Abbie', 'Hoffman', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0], 'lang': 'en'}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Mismatch in length\n\nlen(example['ner_tags']), len(tokenized_example['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:15:08.772471Z","iopub.execute_input":"2023-12-04T09:15:08.772823Z","iopub.status.idle":"2023-12-04T09:15:08.778969Z","shell.execute_reply.started":"2023-12-04T09:15:08.772798Z","shell.execute_reply":"2023-12-04T09:15:08.778044Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"(12, 16)"},"metadata":{}}]},{"cell_type":"code","source":"# This is due to the presence of special tokens and subwords (typical of BERT), as figured in this example\n\ntokens","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:15:13.079863Z","iopub.execute_input":"2023-12-04T09:15:13.080217Z","iopub.status.idle":"2023-12-04T09:15:13.086309Z","shell.execute_reply.started":"2023-12-04T09:15:13.080187Z","shell.execute_reply":"2023-12-04T09:15:13.085349Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['[CLS]',\n 'The',\n 'campaign',\n 'was',\n 'organized',\n ',',\n 'among',\n 'others',\n ',',\n 'by',\n 'A',\n '##bb',\n '##ie',\n 'Hoffman',\n '.',\n '[SEP]']"},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_align(example, label_all_tokens=True):\n    '''\n    Tokenize and align the NER labels with the corresponding tokens in a given example.\n\n    Args:\n        example (dict): A dictionary containing 'tokens' and 'ner_tags' for a single example.\n        label_all_tokens (bool, optional): If True, assigns NER labels to all tokens; if False, assigns labels only to the first token of each word.\n\n    Returns:\n        dict: Tokenized input with aligned NER labels.\n\n    Note:\n        Special tokens are marked with -100, and this will be ignored by PyTorch during training.\n        Labels are aligned to their corresponding tokens, considering word boundaries and the label_all_tokens flag.\n\n    '''\n    \n    tokenized_input = tokenizer(example['tokens'], truncation=True, is_split_into_words=True)\n    labels = [] \n    \n    for i, label in enumerate(example['ner_tags']):\n        word_ids = tokenized_input.word_ids(batch_index=i)\n        \n        previous_word_idx = None\n        label_ids = []\n        \n        for word_idx in word_ids:\n            if word_idx is None:\n                label_ids.append(-100)  # Special tokens will be ignored during training\n            \n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])  # Assign NER label to the token\n            \n            else:\n                # If word_idx is not a special token,\n                # append the NER tag if label_all_tokens is true, else assign -100\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n                    \n            previous_word_idx = word_idx\n            \n        labels.append(label_ids)\n    \n    # Add processed labels to the tokenized input dictionary\n    tokenized_input['labels'] = labels\n    \n    return tokenized_input","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:19:54.548543Z","iopub.execute_input":"2023-12-04T15:19:54.549504Z","iopub.status.idle":"2023-12-04T15:19:54.560694Z","shell.execute_reply.started":"2023-12-04T15:19:54.549460Z","shell.execute_reply":"2023-12-04T15:19:54.559781Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# System A","metadata":{}},{"cell_type":"code","source":"# We apply the logic to entire dataset\n\ntokenized_data = multinerd.map(tokenize_align, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:15:39.459102Z","iopub.execute_input":"2023-12-04T09:15:39.459456Z","iopub.status.idle":"2023-12-04T09:16:07.635170Z","shell.execute_reply.started":"2023-12-04T09:15:39.459429Z","shell.execute_reply":"2023-12-04T09:16:07.634357Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/106 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f306a938671429991c39d63f41d3413"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/27 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"115c5c18b66f4f0fa85eff1ac500daa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e669522763fd4a998f91007bc4b5feea"}},"metadata":{}}]},{"cell_type":"code","source":"# Import model from HuggingFace\n\nmodel_a = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=31)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:18:36.815054Z","iopub.execute_input":"2023-12-04T09:18:36.816140Z","iopub.status.idle":"2023-12-04T09:18:40.340633Z","shell.execute_reply.started":"2023-12-04T09:18:36.816099Z","shell.execute_reply":"2023-12-04T09:18:40.339809Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbe4ef6ce3154cbfa54f61b8d6baec5f"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# We use the data collator to pad tokens so all samples are of same length\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:29.812260Z","iopub.execute_input":"2023-12-04T15:24:29.812949Z","iopub.status.idle":"2023-12-04T15:24:29.817438Z","shell.execute_reply.started":"2023-12-04T15:24:29.812915Z","shell.execute_reply":"2023-12-04T15:24:29.816176Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Define training parameters\n\ntraining_args_a = TrainingArguments(\n    output_dir=\"./fine_tune_bert_output_a\",\n    evaluation_strategy=\"steps\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    #logging_steps = 1000,\n    report_to=\"wandb\",\n    run_name = \"RISE_A\",\n    save_strategy=\"no\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:18:46.719226Z","iopub.execute_input":"2023-12-04T09:18:46.719770Z","iopub.status.idle":"2023-12-04T09:18:46.731129Z","shell.execute_reply.started":"2023-12-04T09:18:46.719734Z","shell.execute_reply":"2023-12-04T09:18:46.730217Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Create a mapping between indices and tags for readability purposes\n\nlabel_mapping = {\n    0: \"O\",\n    1: \"B-PER\",\n    2: \"I-PER\",\n    3: \"B-ORG\",\n    4: \"I-ORG\",\n    5: \"B-LOC\",\n    6: \"I-LOC\",\n    7: \"B-ANIM\",\n    8: \"I-ANIM\",\n    9: \"B-BIO\",\n    10: \"I-BIO\",\n    11: \"B-CEL\",\n    12: \"I-CEL\",\n    13: \"B-DIS\",\n    14: \"I-DIS\",\n    15: \"B-EVE\",\n    16: \"I-EVE\",\n    17: \"B-FOOD\",\n    18: \"I-FOOD\",\n    19: \"B-INST\",\n    20: \"I-INST\",\n    21: \"B-MEDIA\",\n    22: \"I-MEDIA\",\n    23: \"B-MYTH\",\n    24: \"I-MYTH\",\n    25: \"B-PLANT\",\n    26: \"I-PLANT\",\n    27: \"B-TIME\",\n    28: \"I-TIME\",\n    29: \"B-VEHI\",\n    30: \"I-VEHI\",\n}\n\nlabel_names_mapped = [label_mapping[label] for label in label_mapping]\nlen(label_names_mapped)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:19:16.916689Z","iopub.execute_input":"2023-12-04T09:19:16.917304Z","iopub.status.idle":"2023-12-04T09:19:16.927053Z","shell.execute_reply.started":"2023-12-04T09:19:16.917270Z","shell.execute_reply":"2023-12-04T09:19:16.926189Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"31"},"metadata":{}}]},{"cell_type":"code","source":"# seqeval metric from HuggingFace as metrics, which is adequate for chunking tasks such as NER\n\nmetric = datasets.load_metric('seqeval')","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:28:38.820833Z","iopub.execute_input":"2023-12-04T15:28:38.821594Z","iopub.status.idle":"2023-12-04T15:28:39.745507Z","shell.execute_reply.started":"2023-12-04T15:28:38.821559Z","shell.execute_reply":"2023-12-04T15:28:39.744492Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"example = train_dataset[144]\nprint(example)\n\nlabels = [label_names_mapped[i] for i in example['ner_tags']]\nlabels","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:28:18.529767Z","iopub.execute_input":"2023-12-04T15:28:18.530617Z","iopub.status.idle":"2023-12-04T15:28:19.930497Z","shell.execute_reply.started":"2023-12-04T15:28:18.530581Z","shell.execute_reply":"2023-12-04T15:28:19.928666Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7807e2df65964d0dbf472599da8e6a8b"}},"metadata":{}},{"name":"stdout","text":"{'tokens': ['However', ',', 'the', 'plans', 'were', 'revived', 'in', 'August', '2017', ',', 'with', 'the', 'announcement', 'that', 'Paul', 'Scheer', 'would', 'be', 'writing', 'the', 'series', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0], 'lang': 'en'}\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m example \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m144\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(example)\n\u001b[0;32m----> 7\u001b[0m labels \u001b[38;5;241m=\u001b[39m [label_names_mapped[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      8\u001b[0m labels\n","Cell \u001b[0;32mIn[26], line 7\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m example \u001b[38;5;241m=\u001b[39m train_dataset[\u001b[38;5;241m144\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(example)\n\u001b[0;32m----> 7\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[43mlabel_names_mapped\u001b[49m[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner_tags\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m      8\u001b[0m labels\n","\u001b[0;31mNameError\u001b[0m: name 'label_names_mapped' is not defined"],"ename":"NameError","evalue":"name 'label_names_mapped' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Test the metric on a small sample\n\nmetric.compute(predictions=[labels], references=[labels])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:28:13.032277Z","iopub.execute_input":"2023-12-04T15:28:13.032987Z","iopub.status.idle":"2023-12-04T15:28:13.078672Z","shell.execute_reply.started":"2023-12-04T15:28:13.032954Z","shell.execute_reply":"2023-12-04T15:28:13.077330Z"},"trusted":true},"execution_count":25,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test the metric on a small sample\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmetric\u001b[49m\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39m[labels], references\u001b[38;5;241m=\u001b[39m[labels])\n","\u001b[0;31mNameError\u001b[0m: name 'metric' is not defined"],"ename":"NameError","evalue":"name 'metric' is not defined","output_type":"error"}]},{"cell_type":"code","source":"def compute_metrics_a(eval_preds):\n    \n    '''\n    Custom function for evaluation metrics of the NER task. It computes accuracy, precision, recall, and F1.\n    \n    Parameters:\n        eval_preds: a tuple with predicted logits and true labels\n        \n    Returns:\n        dictionary with defined metrics\n    '''\n    \n    pred_logits, labels = eval_preds\n    pred_logits = np.argmax(pred_logits, axis=2)\n\n    predictions = [\n        [label_mapping[prediction] for (prediction, l) in zip(batch_preds, label) if l != -100] \n        for batch_preds, label in zip(pred_logits, labels)\n    ]\n\n    true_labels = [\n        [label_mapping[l] for (prediction, l) in zip(batch_preds, label) if l != -100] \n        for batch_preds, label in zip(pred_logits, labels)\n    ]\n\n    results = metric.compute(predictions=predictions, references=true_labels)\n\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-04T09:20:06.723393Z","iopub.execute_input":"2023-12-04T09:20:06.724337Z","iopub.status.idle":"2023-12-04T09:20:06.731898Z","shell.execute_reply.started":"2023-12-04T09:20:06.724303Z","shell.execute_reply":"2023-12-04T09:20:06.730796Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Train the model\n# NOTE: we train for 3 epochs because of limited GPU access\n# however, it is advised to train for more epochs in a NER task\n\ntrainer_a = Trainer(\n    model=model_a,\n    args=training_args_a,\n    train_dataset=tokenized_data[\"train\"],\n    eval_dataset=tokenized_data[\"valid\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics_a\n)\n\ntrainer_a.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T10:16:25.783013Z","iopub.execute_input":"2023-12-04T10:16:25.783841Z","iopub.status.idle":"2023-12-04T11:24:38.579117Z","shell.execute_reply.started":"2023-12-04T10:16:25.783811Z","shell.execute_reply":"2023-12-04T11:24:38.577948Z"},"trusted":true},"execution_count":38,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19692' max='19692' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19692/19692 1:08:10, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1000</td>\n      <td>0.029000</td>\n      <td>0.089517</td>\n      <td>0.861586</td>\n      <td>0.883222</td>\n      <td>0.872270</td>\n      <td>0.973968</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.026800</td>\n      <td>0.089896</td>\n      <td>0.880721</td>\n      <td>0.861038</td>\n      <td>0.870768</td>\n      <td>0.974299</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.026700</td>\n      <td>0.086117</td>\n      <td>0.866047</td>\n      <td>0.884432</td>\n      <td>0.875143</td>\n      <td>0.974454</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.022700</td>\n      <td>0.087909</td>\n      <td>0.871454</td>\n      <td>0.880697</td>\n      <td>0.876051</td>\n      <td>0.974842</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.021700</td>\n      <td>0.095486</td>\n      <td>0.862138</td>\n      <td>0.889571</td>\n      <td>0.875640</td>\n      <td>0.974606</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.022100</td>\n      <td>0.089360</td>\n      <td>0.861895</td>\n      <td>0.894098</td>\n      <td>0.877701</td>\n      <td>0.974890</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.017500</td>\n      <td>0.109107</td>\n      <td>0.865016</td>\n      <td>0.884477</td>\n      <td>0.874638</td>\n      <td>0.974406</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.013500</td>\n      <td>0.108014</td>\n      <td>0.860411</td>\n      <td>0.891633</td>\n      <td>0.875744</td>\n      <td>0.974283</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.013500</td>\n      <td>0.109042</td>\n      <td>0.853810</td>\n      <td>0.902165</td>\n      <td>0.877322</td>\n      <td>0.974230</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.013400</td>\n      <td>0.107089</td>\n      <td>0.869591</td>\n      <td>0.889571</td>\n      <td>0.879468</td>\n      <td>0.975369</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.014600</td>\n      <td>0.103186</td>\n      <td>0.877841</td>\n      <td>0.885553</td>\n      <td>0.881680</td>\n      <td>0.975585</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.014300</td>\n      <td>0.102159</td>\n      <td>0.868368</td>\n      <td>0.895143</td>\n      <td>0.881552</td>\n      <td>0.975555</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.013000</td>\n      <td>0.101468</td>\n      <td>0.890188</td>\n      <td>0.880533</td>\n      <td>0.885334</td>\n      <td>0.976339</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.007300</td>\n      <td>0.119212</td>\n      <td>0.881010</td>\n      <td>0.887973</td>\n      <td>0.884478</td>\n      <td>0.976093</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.007100</td>\n      <td>0.117724</td>\n      <td>0.876763</td>\n      <td>0.892559</td>\n      <td>0.884590</td>\n      <td>0.975749</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.011500</td>\n      <td>0.109641</td>\n      <td>0.872730</td>\n      <td>0.897593</td>\n      <td>0.884987</td>\n      <td>0.975885</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.011400</td>\n      <td>0.107381</td>\n      <td>0.886577</td>\n      <td>0.888047</td>\n      <td>0.887312</td>\n      <td>0.976574</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.010000</td>\n      <td>0.111261</td>\n      <td>0.878412</td>\n      <td>0.892873</td>\n      <td>0.885583</td>\n      <td>0.976244</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.011600</td>\n      <td>0.108096</td>\n      <td>0.878629</td>\n      <td>0.894800</td>\n      <td>0.886641</td>\n      <td>0.976336</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluate on test set \n\nresults = trainer_a.predict(tokenized_data[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T11:24:38.580885Z","iopub.execute_input":"2023-12-04T11:24:38.581192Z","iopub.status.idle":"2023-12-04T11:25:29.066850Z","shell.execute_reply.started":"2023-12-04T11:24:38.581163Z","shell.execute_reply":"2023-12-04T11:25:29.065745Z"},"trusted":true},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"pred_logits, labels = results.predictions, results.label_ids\npred_logits = np.argmax(pred_logits, axis=2)\n\n# Flatten the predictions and true labels\nflat_true_labels = [label for sequence_labels in labels for label in sequence_labels]\nflat_pred_labels = [label for sequence_labels in pred_logits for label in sequence_labels]\n\n# Ignore special tokens (-100) during evaluation\nvalid_indices = [index for index, label in enumerate(flat_true_labels) if label != -100]\n\n# Use the sklearn classification_report function with label_mapping\nreport = classification_report(\n    np.array(flat_true_labels)[valid_indices],\n    np.array(flat_pred_labels)[valid_indices],\n    target_names=label_names_mapped\n)\n\nprint(report)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T11:25:29.068305Z","iopub.execute_input":"2023-12-04T11:25:29.068707Z","iopub.status.idle":"2023-12-04T11:25:31.540035Z","shell.execute_reply.started":"2023-12-04T11:25:29.068667Z","shell.execute_reply":"2023-12-04T11:25:31.538921Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           O       0.99      0.99      0.99    329987\n       B-PER       0.99      0.99      0.99      7354\n       I-PER       1.00      1.00      1.00     11509\n       B-ORG       0.98      0.98      0.98      5131\n       I-ORG       0.99      0.99      0.99      5693\n       B-LOC       1.00      0.99      0.99     19489\n       I-LOC       0.99      0.99      0.99      7129\n      B-ANIM       0.73      0.79      0.76      3351\n      I-ANIM       0.70      0.77      0.73      1658\n       B-BIO       0.42      0.74      0.54        34\n       I-BIO       0.00      0.00      0.00         0\n       B-CEL       0.78      0.93      0.85        56\n       I-CEL       0.90      0.82      0.86        22\n       B-DIS       0.78      0.82      0.80      2014\n       I-DIS       0.76      0.78      0.77       917\n       B-EVE       0.96      0.96      0.96       451\n       I-EVE       0.96      0.98      0.97       667\n      B-FOOD       0.68      0.70      0.69      1107\n      I-FOOD       0.58      0.61      0.59       234\n      B-INST       0.50      0.72      0.59        25\n      I-INST       0.62      0.62      0.62        16\n     B-MEDIA       0.93      0.97      0.95       653\n     I-MEDIA       0.98      0.98      0.98       989\n      B-MYTH       0.82      0.82      0.82        60\n      I-MYTH       0.77      0.77      0.77        13\n     B-PLANT       0.69      0.78      0.73      1933\n     I-PLANT       0.56      0.63      0.59       842\n      B-TIME       0.83      0.88      0.86       422\n      I-TIME       0.88      0.83      0.85       234\n      B-VEHI       0.83      0.91      0.87        58\n      I-VEHI       0.90      0.95      0.92        56\n\n    accuracy                           0.98    402104\n   macro avg       0.79      0.83      0.81    402104\nweighted avg       0.98      0.98      0.98    402104\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# System B","metadata":{}},{"cell_type":"code","source":"# Change the original dataset as to retrieve only the entities we need\n# while setting all other entities to 0\n\nvalid_ner_tags = [1, 2, 3, 4, 5, 6, 7, 8, 13, 14]\n\n# Define a function to modify ner_tags in each sample\ndef modify_ner_tags(sample):\n    # Iterate through ner_tags and replace invalid values with 0\n    sample['ner_tags'] = [tag if tag in valid_ner_tags else 0 for tag in sample['ner_tags']]\n    return sample\n\n# Apply the modification to the entire dataset\nmodified_dataset = multinerd.map(modify_ner_tags)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:19:59.777519Z","iopub.execute_input":"2023-12-04T15:19:59.777890Z","iopub.status.idle":"2023-12-04T15:20:25.058480Z","shell.execute_reply.started":"2023-12-04T15:19:59.777860Z","shell.execute_reply":"2023-12-04T15:20:25.057745Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/105024 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0d2b4e652f4d24bee10dd10cc23fc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26256 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d574ba905c3146b9bf540a62e1c5576b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/16454 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8869e80366e140cb985dc65679414e0b"}},"metadata":{}}]},{"cell_type":"code","source":"# Apply the logic to entire dataset\n\ntokenized_data_b = modified_dataset.map(tokenize_align, batched=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:23:01.918739Z","iopub.execute_input":"2023-12-04T15:23:01.919508Z","iopub.status.idle":"2023-12-04T15:23:27.910379Z","shell.execute_reply.started":"2023-12-04T15:23:01.919475Z","shell.execute_reply":"2023-12-04T15:23:27.909459Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/106 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15568829189a484a8db47a73684fdde9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/27 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3473d0587cca47ecbcd0acace0d4147a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/17 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff76baa68e5464687beac71f01d0af5"}},"metadata":{}}]},{"cell_type":"code","source":"# Create another label mapping but with the needed tags only\n\nnew_label_mapping = {\n    0: \"O\",\n    1: \"B-PER\",\n    2: \"I-PER\",\n    3: \"B-ORG\",\n    4: \"I-ORG\",\n    5: \"B-LOC\",\n    6: \"I-LOC\",\n    7: \"B-ANIM\",\n    8: \"I-ANIM\",\n    13: \"B-DIS\",\n    14: \"I-DIS\",\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:22:13.620425Z","iopub.execute_input":"2023-12-04T15:22:13.621337Z","iopub.status.idle":"2023-12-04T15:22:13.626400Z","shell.execute_reply.started":"2023-12-04T15:22:13.621275Z","shell.execute_reply":"2023-12-04T15:22:13.625344Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Redefine metric custom function with new dictionary\n\ndef compute_metrics_b(eval_preds):\n    pred_logits, labels = eval_preds\n    pred_logits = np.argmax(pred_logits, axis=2)\n\n    predictions = [\n        [new_label_mapping[prediction] for (prediction, l) in zip(batch_preds, label) if l != -100] \n        for batch_preds, label in zip(pred_logits, labels)\n    ]\n\n    true_labels = [\n        [new_label_mapping[l] for (prediction, l) in zip(batch_preds, label) if l != -100] \n        for batch_preds, label in zip(pred_logits, labels)\n    ]\n\n    results = metric.compute(predictions=predictions, references=true_labels)\n\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:22:16.434475Z","iopub.execute_input":"2023-12-04T15:22:16.434834Z","iopub.status.idle":"2023-12-04T15:22:16.442642Z","shell.execute_reply.started":"2023-12-04T15:22:16.434806Z","shell.execute_reply":"2023-12-04T15:22:16.441612Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"training_args_b = TrainingArguments(\n    output_dir=\"./fine_tune_bert_output_b\",\n    evaluation_strategy=\"steps\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    #logging_steps = 1000,\n    report_to=\"wandb\",\n    run_name = \"RISE_B\",\n    save_strategy=\"no\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:23:44.087030Z","iopub.execute_input":"2023-12-04T15:23:44.087450Z","iopub.status.idle":"2023-12-04T15:23:44.093590Z","shell.execute_reply.started":"2023-12-04T15:23:44.087411Z","shell.execute_reply":"2023-12-04T15:23:44.092540Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model_b = AutoModelForTokenClassification.from_pretrained('bert-base-cased', num_labels=15)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:22:34.456532Z","iopub.execute_input":"2023-12-04T15:22:34.457135Z","iopub.status.idle":"2023-12-04T15:22:44.551076Z","shell.execute_reply.started":"2023-12-04T15:22:34.457099Z","shell.execute_reply":"2023-12-04T15:22:44.550135Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00ae079410a64247ac068d6909538d53"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer_b = Trainer(\n    model=model_b,\n    args=training_args_b,\n    train_dataset=tokenized_data_b[\"train\"],\n    eval_dataset=tokenized_data_b[\"valid\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics_b\n)\n\ntrainer_b.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:28:43.814316Z","iopub.execute_input":"2023-12-04T15:28:43.815177Z","iopub.status.idle":"2023-12-04T16:01:39.320143Z","shell.execute_reply.started":"2023-12-04T15:28:43.815134Z","shell.execute_reply":"2023-12-04T16:01:39.319210Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6564' max='6564' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6564/6564 32:54, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.036300</td>\n      <td>0.080751</td>\n      <td>0.792136</td>\n      <td>0.921323</td>\n      <td>0.851860</td>\n      <td>0.972556</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.053000</td>\n      <td>0.049201</td>\n      <td>0.894481</td>\n      <td>0.895352</td>\n      <td>0.894916</td>\n      <td>0.982548</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.046300</td>\n      <td>0.048508</td>\n      <td>0.877449</td>\n      <td>0.907296</td>\n      <td>0.892123</td>\n      <td>0.981843</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.045300</td>\n      <td>0.043622</td>\n      <td>0.913085</td>\n      <td>0.894397</td>\n      <td>0.903644</td>\n      <td>0.983928</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.041600</td>\n      <td>0.043664</td>\n      <td>0.903291</td>\n      <td>0.909972</td>\n      <td>0.906619</td>\n      <td>0.984174</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.039900</td>\n      <td>0.041305</td>\n      <td>0.908576</td>\n      <td>0.908194</td>\n      <td>0.908385</td>\n      <td>0.984598</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.036400</td>\n      <td>0.039580</td>\n      <td>0.901742</td>\n      <td>0.922852</td>\n      <td>0.912175</td>\n      <td>0.985142</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.035500</td>\n      <td>0.038591</td>\n      <td>0.919055</td>\n      <td>0.909360</td>\n      <td>0.914182</td>\n      <td>0.985418</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.035300</td>\n      <td>0.037028</td>\n      <td>0.927853</td>\n      <td>0.913029</td>\n      <td>0.920381</td>\n      <td>0.986261</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.032500</td>\n      <td>0.037029</td>\n      <td>0.917010</td>\n      <td>0.924247</td>\n      <td>0.920614</td>\n      <td>0.986294</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.034200</td>\n      <td>0.036466</td>\n      <td>0.907818</td>\n      <td>0.932713</td>\n      <td>0.920097</td>\n      <td>0.986103</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.032600</td>\n      <td>0.035763</td>\n      <td>0.916056</td>\n      <td>0.929273</td>\n      <td>0.922617</td>\n      <td>0.986556</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.031800</td>\n      <td>0.035195</td>\n      <td>0.919286</td>\n      <td>0.926559</td>\n      <td>0.922909</td>\n      <td>0.986657</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=6564, training_loss=0.03843973269192349, metrics={'train_runtime': 1975.0159, 'train_samples_per_second': 53.176, 'train_steps_per_second': 3.324, 'total_flos': 3033089085287520.0, 'train_loss': 0.03843973269192349, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"results_b = trainer_b.predict(tokenized_data_b[\"test\"])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T16:01:51.082361Z","iopub.execute_input":"2023-12-04T16:01:51.082750Z","iopub.status.idle":"2023-12-04T16:02:42.206385Z","shell.execute_reply.started":"2023-12-04T16:01:51.082717Z","shell.execute_reply":"2023-12-04T16:02:42.205321Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"cell_type":"code","source":"pred_logits, labels = results_b.predictions, results_b.label_ids\npred_logits = np.argmax(pred_logits, axis=2)\n\n# Flatten the predictions and true labels\nflat_true_labels = [label for sequence_labels in labels for label in sequence_labels]\nflat_pred_labels = [label for sequence_labels in pred_logits for label in sequence_labels]\n\n# Ignore special tokens (-100) during evaluation\nvalid_indices = [index for index, label in enumerate(flat_true_labels) if label != -100]\n\n# Use the sklearn classification_report function with label_mapping\nreport_b = classification_report(\n    np.array(flat_true_labels)[valid_indices],\n    np.array(flat_pred_labels)[valid_indices],\n    target_names=list(new_label_mapping.values())\n)\n\nprint(report_b)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T16:04:50.287881Z","iopub.execute_input":"2023-12-04T16:04:50.288300Z","iopub.status.idle":"2023-12-04T16:04:52.599722Z","shell.execute_reply.started":"2023-12-04T16:04:50.288252Z","shell.execute_reply":"2023-12-04T16:04:52.598748Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           O       1.00      0.99      0.99    337859\n       B-PER       0.99      0.99      0.99      7354\n       I-PER       1.00      1.00      1.00     11509\n       B-ORG       0.98      0.98      0.98      5131\n       I-ORG       0.99      0.99      0.99      5693\n       B-LOC       1.00      0.99      1.00     19489\n       I-LOC       0.99      1.00      0.99      7129\n      B-ANIM       0.71      0.80      0.75      3351\n      I-ANIM       0.67      0.79      0.73      1658\n       B-DIS       0.77      0.80      0.78      2014\n       I-DIS       0.74      0.76      0.75       917\n\n    accuracy                           0.99    402104\n   macro avg       0.89      0.92      0.90    402104\nweighted avg       0.99      0.99      0.99    402104\n\n","output_type":"stream"}]}]}